{% set name = "deepspeed" %}
{% set version = "0.15.3" %}
{% set build = 1 %}

{% set torch_proc_type = "cuda" if cuda_compiler_version != "None" else "cpu" %}

# Build number trick to make sure CUDA builds are still picked
{% if cuda_compiler_version != "None" %}
{% set build = build + 200 %}
{% endif %}

{% if cuda_compiler_version in (None, "None", True, False) %}
{% set cuda_major = 0 %}
{% else %}
{% set cuda_major = environ.get("cuda_compiler_version", "11.2").split(".")[0] | int %}
{% endif %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://pypi.org/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.tar.gz
  sha256: d7f9821e4d6fc7c52191362c8338ea29fc435d81d4bf1a37bf87d72db32ff42a

build:
  number: {{ build }}
  string: cuda{{ cuda_compiler_version | replace('.', '') }}py{{ CONDA_PY }}h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}  # [cuda_compiler_version != "None"]
  string: cpu_py{{ CONDA_PY }}h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}  # [cuda_compiler_version == "None"]
  skip: true  # [win]

requirements:
  # NOTE(hadim): pytorch is only needed during the build in reqs.build and reqs.host when the ops are being built which
  # is only possible when CUDA is available. To prevent creating too many builds for pytorch-cpu we remove the pytorch
  # requirements when CUDA is not available.

  build:
    - cross-python_{{ target_platform }}               # [build_platform != target_platform]
    - pip
    - python                                           # [build_platform != target_platform]
    - pytorch                                          # [(build_platform != target_platform) and cuda_compiler_version not in (undefined, 'None')]
    - setuptools
    - {{ compiler('c') }}
    - {{ compiler('cxx') }}
    - {{ compiler('cuda') }}                           # [cuda_compiler_version not in (undefined, 'None')]
    - {{ stdlib("c") }}
    {% if cuda_major >= 12 %}
    - cuda-profiler-api                                # [build_platform != target_platform]
    - libcublas-dev                                    # [build_platform != target_platform]
    - libcurand-dev                                    # [build_platform != target_platform]
    - libcusolver-dev                                  # [build_platform != target_platform]
    - libcusparse-dev                                  # [build_platform != target_platform]
    {% endif %}
  host:
    - git
    # For the `async_io` op
    - libaio                                           # [linux]
    - oneccl-devel                                     # [cuda_compiler_version not in (undefined, 'None')]
    - pip
    - py-cpuinfo
    - python
    # Leaving two dependencies helps rerender correctly
    # The first gets filled in by the global pinnings
    # The second gets the processor type
    - pytorch                                          # [cuda_compiler_version not in (undefined, 'None')]
    - pytorch =*={{ torch_proc_type }}*                # [cuda_compiler_version not in (undefined, 'None')]
    - setuptools
    {% if cuda_major >= 12 %}
    - cuda-profiler-api
    - libcublas-dev
    - libcurand-dev
    - libcusolver-dev
    - libcusparse-dev
    {% endif %}
  run:
    - hjson-py
    - msgpack-python
    # NOTE(hadim): coming at https://github.com/conda-forge/staged-recipes/pull/19098
    # - ninja-python
    - numpy
    - nvidia-ml-py                                     # [cuda_compiler_version not in (undefined, 'None')]
    - packaging >=20.0
    - psutil
    - py-cpuinfo
    - pydantic >=2.0.0
    - python
    - pytorch                                          # [cuda_compiler_version in (undefined, 'None')]
    - setuptools
    - tqdm
  run_constrained:
    # 2022/02/05 hmaarrfk
    # While conda packaging seems to allow us to specify
    # constraints on the same package in different lines
    # the resulting package doesn't have the ability to
    # be specified in multiples lines
    # This makes it tricky to use run_exports
    # we add the GPU constraint in the run_constrained
    # to allow us to have "two" constraints on the
    # running package
    - pytorch =*={{ torch_proc_type }}*               # [cuda_compiler_version not in (undefined, 'None')]

test:
  imports:
    - deepspeed
  commands:
    # NOTE(hadim): need ninja-python for the pip check to pass
    # - pip check
    - ds_report
    - deepspeed --help
  requires:
    - pip

about:
  home: http://deepspeed.ai
  summary: DeepSpeed library
  dev_url: https://github.com/microsoft/DeepSpeed
  license: Apache-2.0
  license_file: LICENSE

extra:
  recipe-maintainers:
    - weiji14
    - loadams
